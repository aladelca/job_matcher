{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import docx\n",
    "import stanza\n",
    "stanza.download('es')\n",
    "def clean_text(texto):\n",
    "    text_ascii = unidecode(texto)\n",
    "    result = re.sub(r'[^A-Za-z0-9 ]', '', text_ascii)\n",
    "    result = result.lower()\n",
    "    return result\n",
    "\n",
    "def get_tokens(\n",
    "    df : pd.DataFrame,\n",
    "    text_col : str\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[text_col] = df[text_col].apply(lambda x: word_tokenize(x))\n",
    "    return df\n",
    "\n",
    "def remove_stopwords(\n",
    "    df : pd.DataFrame,\n",
    "    stopwords : list,\n",
    "    tokens_col : str\n",
    "    ) -> pd.DataFrame:\n",
    "    df[tokens_col] = df[tokens_col].apply(lambda x: [word for word in x if word not in stopwords])\n",
    "    return df\n",
    "\n",
    "def lemmatize_stanza(text_list):\n",
    "    nlp = stanza.Pipeline('es')\n",
    "    final = []\n",
    "    for text in text_list:\n",
    "        doc = nlp(text)\n",
    "        for sentence in doc.sentences:\n",
    "            for word in sentence.words:\n",
    "                final.append(word.lemma)\n",
    "    return final\n",
    "\n",
    "def lemmatize_tokens(\n",
    "    df : pd.DataFrame,\n",
    "    tokens_col : str\n",
    "    ) -> pd.DataFrame:\n",
    "    def lemmatize(text):\n",
    "        nlp = spacy.load(\"es_core_news_sm\")\n",
    "        return nlp(text)[0].lemma_\n",
    "    df[tokens_col] = df[tokens_col].apply(lambda x: lemmatize_stanza(x))\n",
    "    return df\n",
    "\n",
    "def get_text(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "\n",
    "def preprocess_text(\n",
    "        df : pd.DataFrame,\n",
    "        text_col : str,\n",
    "        stopwords : list\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "    df = df.copy()\n",
    "    df[text_col] = df[text_col].apply(clean_text)\n",
    "    df = get_tokens(df, text_col)\n",
    "    df = remove_stopwords(df, stopwords, text_col)\n",
    "    df = lemmatize_tokens(df, text_col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = get_text(\"/Users/aladelca/Downloads/Carlos Adrián Alarcón Delgado alarcon.docx\")\n",
    "df = pd.DataFrame({\"texto\": [texto]})\n",
    "df = preprocess_text(\n",
    "    df,\n",
    "    \"texto\",\n",
    "    list(set(stopwords.words('spanish')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"final\"] = df[\"texto\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "vect = pickle.load(open(\"../job_matcher/static/data/job_vectorizer.pickle\", \"rb\"))\n",
    "jobs = pickle.load(open(\"../job_matcher/static/data/puestos.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "df_similarity = pd.DataFrame(cosine_similarity(vect.transform(jobs[\"PUESTO\"]),vect.transform(df[\"final\"])), columns=[\"similarity\"])\n",
    "index_selected = df_similarity.sort_values(by=\"similarity\", ascending=False).head(5).index\n",
    "jobs.loc[index_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(jobs[\"PUESTO\"].shape)\n",
    "print(df[\"final\"].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
