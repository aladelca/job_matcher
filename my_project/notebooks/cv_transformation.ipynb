{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 53.0MB/s]                    \n",
      "2025-03-01 15:25:18 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:25:18 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2025-03-01 15:25:19 INFO: File exists: /Users/aladelca/stanza_resources/es/default.zip\n",
      "2025-03-01 15:25:21 INFO: Finished downloading models and saved to /Users/aladelca/stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import docx\n",
    "import stanza\n",
    "stanza.download('es')\n",
    "def clean_text(texto):\n",
    "    text_ascii = unidecode(texto)\n",
    "    result = re.sub(r'[^A-Za-z0-9 ]', '', text_ascii)\n",
    "    result = result.lower()\n",
    "    return result\n",
    "\n",
    "def get_tokens(\n",
    "    df : pd.DataFrame,\n",
    "    text_col : str\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[text_col] = df[text_col].apply(lambda x: word_tokenize(x))\n",
    "    return df\n",
    "\n",
    "def remove_stopwords(\n",
    "    df : pd.DataFrame,\n",
    "    stopwords : list,\n",
    "    tokens_col : str\n",
    "    ) -> pd.DataFrame:\n",
    "    df[tokens_col] = df[tokens_col].apply(lambda x: [word for word in x if word not in stopwords])\n",
    "    return df\n",
    "\n",
    "def lemmatize_stanza(text_list):\n",
    "    nlp = stanza.Pipeline('es')\n",
    "    final = []\n",
    "    for text in text_list:\n",
    "        doc = nlp(text)\n",
    "        for sentence in doc.sentences:\n",
    "            for word in sentence.words:\n",
    "                final.append(word.lemma)\n",
    "    return final\n",
    "\n",
    "def lemmatize_tokens(\n",
    "    df : pd.DataFrame,\n",
    "    tokens_col : str\n",
    "    ) -> pd.DataFrame:\n",
    "    def lemmatize(text):\n",
    "        nlp = spacy.load(\"es_core_news_sm\")\n",
    "        return nlp(text)[0].lemma_\n",
    "    df[tokens_col] = df[tokens_col].apply(lambda x: lemmatize_stanza(x))\n",
    "    return df\n",
    "\n",
    "def get_text(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "\n",
    "def preprocess_text(\n",
    "        df : pd.DataFrame,\n",
    "        text_col : str,\n",
    "        stopwords : list\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "    df = df.copy()\n",
    "    df[text_col] = df[text_col].apply(clean_text)\n",
    "    df = get_tokens(df, text_col)\n",
    "    df = remove_stopwords(df, stopwords, text_col)\n",
    "    df = lemmatize_tokens(df, text_col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-01 15:22:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 49.9MB/s]                    \n",
      "2025-03-01 15:22:57 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:22:58 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:22:58 INFO: Using device: cpu\n",
      "2025-03-01 15:22:58 INFO: Loading: tokenize\n",
      "2025-03-01 15:22:58 INFO: Loading: mwt\n",
      "2025-03-01 15:22:58 INFO: Loading: pos\n",
      "2025-03-01 15:22:58 INFO: Loading: lemma\n",
      "2025-03-01 15:22:59 INFO: Loading: constituency\n",
      "2025-03-01 15:22:59 INFO: Loading: depparse\n",
      "2025-03-01 15:22:59 INFO: Loading: sentiment\n",
      "2025-03-01 15:22:59 INFO: Loading: ner\n",
      "2025-03-01 15:23:00 INFO: Done loading processors!\n",
      "2025-03-01 15:23:00 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 44.4MB/s]                    \n",
      "2025-03-01 15:23:00 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:01 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:01 INFO: Using device: cpu\n",
      "2025-03-01 15:23:01 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:01 INFO: Loading: mwt\n",
      "2025-03-01 15:23:02 INFO: Loading: pos\n",
      "2025-03-01 15:23:02 INFO: Loading: lemma\n",
      "2025-03-01 15:23:03 INFO: Loading: constituency\n",
      "2025-03-01 15:23:03 INFO: Loading: depparse\n",
      "2025-03-01 15:23:03 INFO: Loading: sentiment\n",
      "2025-03-01 15:23:03 INFO: Loading: ner\n",
      "2025-03-01 15:23:04 INFO: Done loading processors!\n",
      "2025-03-01 15:23:04 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 60.3MB/s]                    \n",
      "2025-03-01 15:23:04 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:05 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:05 INFO: Using device: cpu\n",
      "2025-03-01 15:23:05 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:05 INFO: Loading: mwt\n",
      "2025-03-01 15:23:05 INFO: Loading: pos\n",
      "2025-03-01 15:23:06 INFO: Loading: lemma\n",
      "2025-03-01 15:23:07 INFO: Loading: constituency\n",
      "2025-03-01 15:23:07 INFO: Loading: depparse\n",
      "2025-03-01 15:23:07 INFO: Loading: sentiment\n",
      "2025-03-01 15:23:07 INFO: Loading: ner\n",
      "2025-03-01 15:23:08 INFO: Done loading processors!\n",
      "2025-03-01 15:23:08 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 51.0MB/s]                    \n",
      "2025-03-01 15:23:08 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:09 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:09 INFO: Using device: cpu\n",
      "2025-03-01 15:23:09 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:09 INFO: Loading: mwt\n",
      "2025-03-01 15:23:09 INFO: Loading: pos\n",
      "2025-03-01 15:23:10 INFO: Loading: lemma\n",
      "2025-03-01 15:23:10 INFO: Loading: constituency\n",
      "2025-03-01 15:23:11 INFO: Loading: depparse\n",
      "2025-03-01 15:23:11 INFO: Loading: sentiment\n",
      "2025-03-01 15:23:11 INFO: Loading: ner\n",
      "2025-03-01 15:23:12 INFO: Done loading processors!\n",
      "2025-03-01 15:23:12 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 44.7MB/s]                    \n",
      "2025-03-01 15:23:12 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:13 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:13 INFO: Using device: cpu\n",
      "2025-03-01 15:23:13 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:13 INFO: Loading: mwt\n",
      "2025-03-01 15:23:13 INFO: Loading: pos\n",
      "2025-03-01 15:23:14 INFO: Loading: lemma\n",
      "2025-03-01 15:23:14 INFO: Loading: constituency\n",
      "2025-03-01 15:23:15 INFO: Loading: depparse\n",
      "2025-03-01 15:23:15 INFO: Loading: sentiment\n",
      "2025-03-01 15:23:15 INFO: Loading: ner\n",
      "2025-03-01 15:23:16 INFO: Done loading processors!\n",
      "2025-03-01 15:23:16 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 48.4MB/s]                    \n",
      "2025-03-01 15:23:16 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:17 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:17 INFO: Using device: cpu\n",
      "2025-03-01 15:23:17 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:17 INFO: Loading: mwt\n",
      "2025-03-01 15:23:17 INFO: Loading: pos\n",
      "2025-03-01 15:23:18 INFO: Loading: lemma\n",
      "2025-03-01 15:23:19 INFO: Loading: constituency\n",
      "2025-03-01 15:23:19 INFO: Loading: depparse\n",
      "2025-03-01 15:23:19 INFO: Loading: sentiment\n",
      "2025-03-01 15:23:19 INFO: Loading: ner\n",
      "2025-03-01 15:23:20 INFO: Done loading processors!\n",
      "2025-03-01 15:23:20 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 55.8MB/s]                    \n",
      "2025-03-01 15:23:20 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:21 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:21 INFO: Using device: cpu\n",
      "2025-03-01 15:23:21 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:21 INFO: Loading: mwt\n",
      "2025-03-01 15:23:21 INFO: Loading: pos\n",
      "2025-03-01 15:23:22 INFO: Loading: lemma\n",
      "2025-03-01 15:23:22 INFO: Loading: constituency\n",
      "2025-03-01 15:23:23 INFO: Loading: depparse\n",
      "2025-03-01 15:23:23 INFO: Loading: sentiment\n",
      "2025-03-01 15:23:23 INFO: Loading: ner\n",
      "2025-03-01 15:23:24 INFO: Done loading processors!\n",
      "2025-03-01 15:23:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 67.1MB/s]                    \n",
      "2025-03-01 15:23:24 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:25 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:25 INFO: Using device: cpu\n",
      "2025-03-01 15:23:25 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:25 INFO: Loading: mwt\n",
      "2025-03-01 15:23:25 INFO: Loading: pos\n",
      "2025-03-01 15:23:26 INFO: Loading: lemma\n",
      "2025-03-01 15:23:26 INFO: Loading: constituency\n",
      "2025-03-01 15:23:27 INFO: Loading: depparse\n",
      "2025-03-01 15:23:27 INFO: Loading: sentiment\n",
      "2025-03-01 15:23:27 INFO: Loading: ner\n",
      "2025-03-01 15:23:28 INFO: Done loading processors!\n",
      "2025-03-01 15:23:28 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 50.2MB/s]                    \n",
      "2025-03-01 15:23:28 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:29 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:29 INFO: Using device: cpu\n",
      "2025-03-01 15:23:29 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:29 INFO: Loading: mwt\n",
      "2025-03-01 15:23:29 INFO: Loading: pos\n",
      "2025-03-01 15:23:30 INFO: Loading: lemma\n",
      "2025-03-01 15:23:30 INFO: Loading: constituency\n",
      "2025-03-01 15:23:31 INFO: Loading: depparse\n",
      "2025-03-01 15:23:31 INFO: Loading: sentiment\n",
      "2025-03-01 15:23:31 INFO: Loading: ner\n",
      "2025-03-01 15:23:32 INFO: Done loading processors!\n",
      "2025-03-01 15:23:32 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 48.2MB/s]                    \n",
      "2025-03-01 15:23:32 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:33 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:33 INFO: Using device: cpu\n",
      "2025-03-01 15:23:33 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:33 INFO: Loading: mwt\n",
      "2025-03-01 15:23:33 INFO: Loading: pos\n",
      "2025-03-01 15:23:34 INFO: Loading: lemma\n",
      "2025-03-01 15:23:34 INFO: Loading: constituency\n",
      "2025-03-01 15:23:34 INFO: Loading: depparse\n",
      "2025-03-01 15:23:35 INFO: Loading: sentiment\n",
      "2025-03-01 15:23:35 INFO: Loading: ner\n",
      "2025-03-01 15:23:36 INFO: Done loading processors!\n",
      "2025-03-01 15:23:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 15.6MB/s]                    \n",
      "2025-03-01 15:23:36 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:37 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:37 INFO: Using device: cpu\n",
      "2025-03-01 15:23:37 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:37 INFO: Loading: mwt\n",
      "2025-03-01 15:23:37 INFO: Loading: pos\n",
      "2025-03-01 15:23:38 INFO: Loading: lemma\n",
      "2025-03-01 15:23:38 INFO: Loading: constituency\n",
      "2025-03-01 15:23:38 INFO: Loading: depparse\n",
      "2025-03-01 15:23:39 INFO: Loading: sentiment\n",
      "2025-03-01 15:23:39 INFO: Loading: ner\n",
      "2025-03-01 15:23:40 INFO: Done loading processors!\n",
      "2025-03-01 15:23:40 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 33.0MB/s]                    \n",
      "2025-03-01 15:23:40 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:41 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:41 INFO: Using device: cpu\n",
      "2025-03-01 15:23:41 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:41 INFO: Loading: mwt\n",
      "2025-03-01 15:23:41 INFO: Loading: pos\n",
      "2025-03-01 15:23:42 INFO: Loading: lemma\n",
      "2025-03-01 15:23:42 INFO: Loading: constituency\n",
      "2025-03-01 15:23:42 INFO: Loading: depparse\n",
      "2025-03-01 15:23:43 INFO: Loading: sentiment\n",
      "2025-03-01 15:23:43 INFO: Loading: ner\n",
      "2025-03-01 15:23:44 INFO: Done loading processors!\n",
      "2025-03-01 15:23:44 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 37.6MB/s]                    \n",
      "2025-03-01 15:23:44 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:45 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:45 INFO: Using device: cpu\n",
      "2025-03-01 15:23:45 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:45 INFO: Loading: mwt\n",
      "2025-03-01 15:23:45 INFO: Loading: pos\n",
      "2025-03-01 15:23:46 INFO: Loading: lemma\n",
      "2025-03-01 15:23:46 INFO: Loading: constituency\n",
      "2025-03-01 15:23:46 INFO: Loading: depparse\n",
      "2025-03-01 15:23:47 INFO: Loading: sentiment\n",
      "2025-03-01 15:23:47 INFO: Loading: ner\n",
      "2025-03-01 15:23:48 INFO: Done loading processors!\n",
      "2025-03-01 15:23:48 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 58.0MB/s]                    \n",
      "2025-03-01 15:23:48 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:49 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:49 INFO: Using device: cpu\n",
      "2025-03-01 15:23:49 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:49 INFO: Loading: mwt\n",
      "2025-03-01 15:23:49 INFO: Loading: pos\n",
      "2025-03-01 15:23:50 INFO: Loading: lemma\n",
      "2025-03-01 15:23:51 INFO: Loading: constituency\n",
      "2025-03-01 15:23:52 INFO: Loading: depparse\n",
      "2025-03-01 15:23:52 INFO: Loading: sentiment\n",
      "2025-03-01 15:23:52 INFO: Loading: ner\n",
      "2025-03-01 15:23:53 INFO: Done loading processors!\n",
      "2025-03-01 15:23:53 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 45.0MB/s]                    \n",
      "2025-03-01 15:23:53 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:54 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:54 INFO: Using device: cpu\n",
      "2025-03-01 15:23:54 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:54 INFO: Loading: mwt\n",
      "2025-03-01 15:23:54 INFO: Loading: pos\n",
      "2025-03-01 15:23:55 INFO: Loading: lemma\n",
      "2025-03-01 15:23:55 INFO: Loading: constituency\n",
      "2025-03-01 15:23:56 INFO: Loading: depparse\n",
      "2025-03-01 15:23:56 INFO: Loading: sentiment\n",
      "2025-03-01 15:23:56 INFO: Loading: ner\n",
      "2025-03-01 15:23:57 INFO: Done loading processors!\n",
      "2025-03-01 15:23:57 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 60.9MB/s]                    \n",
      "2025-03-01 15:23:57 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:23:58 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:23:58 INFO: Using device: cpu\n",
      "2025-03-01 15:23:58 INFO: Loading: tokenize\n",
      "2025-03-01 15:23:58 INFO: Loading: mwt\n",
      "2025-03-01 15:23:58 INFO: Loading: pos\n",
      "2025-03-01 15:23:59 INFO: Loading: lemma\n",
      "2025-03-01 15:23:59 INFO: Loading: constituency\n",
      "2025-03-01 15:23:59 INFO: Loading: depparse\n",
      "2025-03-01 15:24:00 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:00 INFO: Loading: ner\n",
      "2025-03-01 15:24:01 INFO: Done loading processors!\n",
      "2025-03-01 15:24:01 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 73.8MB/s]                    \n",
      "2025-03-01 15:24:01 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:02 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:02 INFO: Using device: cpu\n",
      "2025-03-01 15:24:02 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:02 INFO: Loading: mwt\n",
      "2025-03-01 15:24:02 INFO: Loading: pos\n",
      "2025-03-01 15:24:03 INFO: Loading: lemma\n",
      "2025-03-01 15:24:03 INFO: Loading: constituency\n",
      "2025-03-01 15:24:03 INFO: Loading: depparse\n",
      "2025-03-01 15:24:04 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:04 INFO: Loading: ner\n",
      "2025-03-01 15:24:05 INFO: Done loading processors!\n",
      "2025-03-01 15:24:05 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 59.8MB/s]                    \n",
      "2025-03-01 15:24:05 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:06 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:06 INFO: Using device: cpu\n",
      "2025-03-01 15:24:06 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:06 INFO: Loading: mwt\n",
      "2025-03-01 15:24:06 INFO: Loading: pos\n",
      "2025-03-01 15:24:06 INFO: Loading: lemma\n",
      "2025-03-01 15:24:07 INFO: Loading: constituency\n",
      "2025-03-01 15:24:07 INFO: Loading: depparse\n",
      "2025-03-01 15:24:07 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:08 INFO: Loading: ner\n",
      "2025-03-01 15:24:08 INFO: Done loading processors!\n",
      "2025-03-01 15:24:09 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 59.5MB/s]                    \n",
      "2025-03-01 15:24:09 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:10 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:10 INFO: Using device: cpu\n",
      "2025-03-01 15:24:10 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:10 INFO: Loading: mwt\n",
      "2025-03-01 15:24:10 INFO: Loading: pos\n",
      "2025-03-01 15:24:10 INFO: Loading: lemma\n",
      "2025-03-01 15:24:11 INFO: Loading: constituency\n",
      "2025-03-01 15:24:11 INFO: Loading: depparse\n",
      "2025-03-01 15:24:11 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:11 INFO: Loading: ner\n",
      "2025-03-01 15:24:12 INFO: Done loading processors!\n",
      "2025-03-01 15:24:13 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 51.4MB/s]                    \n",
      "2025-03-01 15:24:13 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:14 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:14 INFO: Using device: cpu\n",
      "2025-03-01 15:24:14 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:14 INFO: Loading: mwt\n",
      "2025-03-01 15:24:14 INFO: Loading: pos\n",
      "2025-03-01 15:24:14 INFO: Loading: lemma\n",
      "2025-03-01 15:24:15 INFO: Loading: constituency\n",
      "2025-03-01 15:24:15 INFO: Loading: depparse\n",
      "2025-03-01 15:24:15 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:15 INFO: Loading: ner\n",
      "2025-03-01 15:24:16 INFO: Done loading processors!\n",
      "2025-03-01 15:24:16 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 68.1MB/s]                    \n",
      "2025-03-01 15:24:16 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:17 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:17 INFO: Using device: cpu\n",
      "2025-03-01 15:24:17 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:17 INFO: Loading: mwt\n",
      "2025-03-01 15:24:18 INFO: Loading: pos\n",
      "2025-03-01 15:24:18 INFO: Loading: lemma\n",
      "2025-03-01 15:24:19 INFO: Loading: constituency\n",
      "2025-03-01 15:24:19 INFO: Loading: depparse\n",
      "2025-03-01 15:24:19 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:19 INFO: Loading: ner\n",
      "2025-03-01 15:24:20 INFO: Done loading processors!\n",
      "2025-03-01 15:24:20 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 45.4MB/s]                    \n",
      "2025-03-01 15:24:20 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:21 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:21 INFO: Using device: cpu\n",
      "2025-03-01 15:24:21 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:21 INFO: Loading: mwt\n",
      "2025-03-01 15:24:21 INFO: Loading: pos\n",
      "2025-03-01 15:24:22 INFO: Loading: lemma\n",
      "2025-03-01 15:24:23 INFO: Loading: constituency\n",
      "2025-03-01 15:24:23 INFO: Loading: depparse\n",
      "2025-03-01 15:24:23 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:23 INFO: Loading: ner\n",
      "2025-03-01 15:24:24 INFO: Done loading processors!\n",
      "2025-03-01 15:24:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 44.1MB/s]                    \n",
      "2025-03-01 15:24:24 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:25 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:25 INFO: Using device: cpu\n",
      "2025-03-01 15:24:25 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:25 INFO: Loading: mwt\n",
      "2025-03-01 15:24:25 INFO: Loading: pos\n",
      "2025-03-01 15:24:26 INFO: Loading: lemma\n",
      "2025-03-01 15:24:26 INFO: Loading: constituency\n",
      "2025-03-01 15:24:27 INFO: Loading: depparse\n",
      "2025-03-01 15:24:27 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:27 INFO: Loading: ner\n",
      "2025-03-01 15:24:28 INFO: Done loading processors!\n",
      "2025-03-01 15:24:28 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 50.6MB/s]                    \n",
      "2025-03-01 15:24:28 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:29 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:29 INFO: Using device: cpu\n",
      "2025-03-01 15:24:29 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:29 INFO: Loading: mwt\n",
      "2025-03-01 15:24:29 INFO: Loading: pos\n",
      "2025-03-01 15:24:30 INFO: Loading: lemma\n",
      "2025-03-01 15:24:30 INFO: Loading: constituency\n",
      "2025-03-01 15:24:31 INFO: Loading: depparse\n",
      "2025-03-01 15:24:31 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:31 INFO: Loading: ner\n",
      "2025-03-01 15:24:32 INFO: Done loading processors!\n",
      "2025-03-01 15:24:32 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 53.5MB/s]                    \n",
      "2025-03-01 15:24:32 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:33 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:33 INFO: Using device: cpu\n",
      "2025-03-01 15:24:33 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:33 INFO: Loading: mwt\n",
      "2025-03-01 15:24:33 INFO: Loading: pos\n",
      "2025-03-01 15:24:34 INFO: Loading: lemma\n",
      "2025-03-01 15:24:34 INFO: Loading: constituency\n",
      "2025-03-01 15:24:34 INFO: Loading: depparse\n",
      "2025-03-01 15:24:35 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:35 INFO: Loading: ner\n",
      "2025-03-01 15:24:36 INFO: Done loading processors!\n",
      "2025-03-01 15:24:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 31.0MB/s]                    \n",
      "2025-03-01 15:24:36 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:37 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:37 INFO: Using device: cpu\n",
      "2025-03-01 15:24:37 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:37 INFO: Loading: mwt\n",
      "2025-03-01 15:24:37 INFO: Loading: pos\n",
      "2025-03-01 15:24:38 INFO: Loading: lemma\n",
      "2025-03-01 15:24:38 INFO: Loading: constituency\n",
      "2025-03-01 15:24:38 INFO: Loading: depparse\n",
      "2025-03-01 15:24:39 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:39 INFO: Loading: ner\n",
      "2025-03-01 15:24:40 INFO: Done loading processors!\n",
      "2025-03-01 15:24:40 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 34.1MB/s]                    \n",
      "2025-03-01 15:24:40 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:41 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:41 INFO: Using device: cpu\n",
      "2025-03-01 15:24:41 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:41 INFO: Loading: mwt\n",
      "2025-03-01 15:24:41 INFO: Loading: pos\n",
      "2025-03-01 15:24:42 INFO: Loading: lemma\n",
      "2025-03-01 15:24:42 INFO: Loading: constituency\n",
      "2025-03-01 15:24:42 INFO: Loading: depparse\n",
      "2025-03-01 15:24:43 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:43 INFO: Loading: ner\n",
      "2025-03-01 15:24:44 INFO: Done loading processors!\n",
      "2025-03-01 15:24:44 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 53.9MB/s]                    \n",
      "2025-03-01 15:24:44 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:45 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:45 INFO: Using device: cpu\n",
      "2025-03-01 15:24:45 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:45 INFO: Loading: mwt\n",
      "2025-03-01 15:24:45 INFO: Loading: pos\n",
      "2025-03-01 15:24:46 INFO: Loading: lemma\n",
      "2025-03-01 15:24:46 INFO: Loading: constituency\n",
      "2025-03-01 15:24:46 INFO: Loading: depparse\n",
      "2025-03-01 15:24:46 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:47 INFO: Loading: ner\n",
      "2025-03-01 15:24:48 INFO: Done loading processors!\n",
      "2025-03-01 15:24:48 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 58.5MB/s]                    \n",
      "2025-03-01 15:24:48 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:49 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:49 INFO: Using device: cpu\n",
      "2025-03-01 15:24:49 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:49 INFO: Loading: mwt\n",
      "2025-03-01 15:24:49 INFO: Loading: pos\n",
      "2025-03-01 15:24:50 INFO: Loading: lemma\n",
      "2025-03-01 15:24:50 INFO: Loading: constituency\n",
      "2025-03-01 15:24:50 INFO: Loading: depparse\n",
      "2025-03-01 15:24:50 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:51 INFO: Loading: ner\n",
      "2025-03-01 15:24:52 INFO: Done loading processors!\n",
      "2025-03-01 15:24:52 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 44.1MB/s]                    \n",
      "2025-03-01 15:24:52 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:53 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:53 INFO: Using device: cpu\n",
      "2025-03-01 15:24:53 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:53 INFO: Loading: mwt\n",
      "2025-03-01 15:24:53 INFO: Loading: pos\n",
      "2025-03-01 15:24:53 INFO: Loading: lemma\n",
      "2025-03-01 15:24:54 INFO: Loading: constituency\n",
      "2025-03-01 15:24:54 INFO: Loading: depparse\n",
      "2025-03-01 15:24:54 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:54 INFO: Loading: ner\n",
      "2025-03-01 15:24:55 INFO: Done loading processors!\n",
      "2025-03-01 15:24:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 60.6MB/s]                    \n",
      "2025-03-01 15:24:56 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:24:57 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:24:57 INFO: Using device: cpu\n",
      "2025-03-01 15:24:57 INFO: Loading: tokenize\n",
      "2025-03-01 15:24:57 INFO: Loading: mwt\n",
      "2025-03-01 15:24:57 INFO: Loading: pos\n",
      "2025-03-01 15:24:57 INFO: Loading: lemma\n",
      "2025-03-01 15:24:58 INFO: Loading: constituency\n",
      "2025-03-01 15:24:58 INFO: Loading: depparse\n",
      "2025-03-01 15:24:58 INFO: Loading: sentiment\n",
      "2025-03-01 15:24:58 INFO: Loading: ner\n",
      "2025-03-01 15:24:59 INFO: Done loading processors!\n",
      "2025-03-01 15:24:59 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 35.4MB/s]                    \n",
      "2025-03-01 15:24:59 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:25:00 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:25:00 INFO: Using device: cpu\n",
      "2025-03-01 15:25:00 INFO: Loading: tokenize\n",
      "2025-03-01 15:25:01 INFO: Loading: mwt\n",
      "2025-03-01 15:25:01 INFO: Loading: pos\n",
      "2025-03-01 15:25:01 INFO: Loading: lemma\n",
      "2025-03-01 15:25:03 INFO: Loading: constituency\n",
      "2025-03-01 15:25:04 INFO: Loading: depparse\n",
      "2025-03-01 15:25:04 INFO: Loading: sentiment\n",
      "2025-03-01 15:25:04 INFO: Loading: ner\n",
      "2025-03-01 15:25:05 INFO: Done loading processors!\n",
      "2025-03-01 15:25:05 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 47.8MB/s]                    \n",
      "2025-03-01 15:25:05 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:25:06 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:25:06 INFO: Using device: cpu\n",
      "2025-03-01 15:25:06 INFO: Loading: tokenize\n",
      "2025-03-01 15:25:06 INFO: Loading: mwt\n",
      "2025-03-01 15:25:06 INFO: Loading: pos\n",
      "2025-03-01 15:25:07 INFO: Loading: lemma\n",
      "2025-03-01 15:25:07 INFO: Loading: constituency\n",
      "2025-03-01 15:25:07 INFO: Loading: depparse\n",
      "2025-03-01 15:25:08 INFO: Loading: sentiment\n",
      "2025-03-01 15:25:08 INFO: Loading: ner\n",
      "2025-03-01 15:25:09 INFO: Done loading processors!\n",
      "2025-03-01 15:25:09 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 59.4MB/s]                    \n",
      "2025-03-01 15:25:09 INFO: Downloaded file to /Users/aladelca/stanza_resources/resources.json\n",
      "2025-03-01 15:25:10 INFO: Loading these models for language: es (Spanish):\n",
      "====================================\n",
      "| Processor    | Package           |\n",
      "------------------------------------\n",
      "| tokenize     | combined          |\n",
      "| mwt          | combined          |\n",
      "| pos          | combined_charlm   |\n",
      "| lemma        | combined_nocharlm |\n",
      "| constituency | combined_charlm   |\n",
      "| depparse     | combined_charlm   |\n",
      "| sentiment    | tass2020_charlm   |\n",
      "| ner          | conll02           |\n",
      "====================================\n",
      "\n",
      "2025-03-01 15:25:10 INFO: Using device: cpu\n",
      "2025-03-01 15:25:10 INFO: Loading: tokenize\n",
      "2025-03-01 15:25:10 INFO: Loading: mwt\n",
      "2025-03-01 15:25:10 INFO: Loading: pos\n",
      "2025-03-01 15:25:10 INFO: Loading: lemma\n",
      "2025-03-01 15:25:11 ERROR: Cannot load model from /Users/aladelca/stanza_resources/es/lemma/combined_nocharlm.pt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m texto \u001b[38;5;241m=\u001b[39m get_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/aladelca/Downloads/Carlos Adrián Alarcón Delgado alarcon.docx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtexto\u001b[39m\u001b[38;5;124m\"\u001b[39m: [texto]})\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtexto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspanish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 69\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(df, text_col, stopwords)\u001b[0m\n\u001b[1;32m     67\u001b[0m df \u001b[38;5;241m=\u001b[39m get_tokens(df, text_col)\n\u001b[1;32m     68\u001b[0m df \u001b[38;5;241m=\u001b[39m remove_stopwords(df, stopwords, text_col)\n\u001b[0;32m---> 69\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mlemmatize_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[0;32mIn[21], line 49\u001b[0m, in \u001b[0;36mlemmatize_tokens\u001b[0;34m(df, tokens_col)\u001b[0m\n\u001b[1;32m     47\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes_core_news_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nlp(text)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlemma_\n\u001b[0;32m---> 49\u001b[0m df[tokens_col] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokens_col\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlemmatize_stanza\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-alarcon.adrianc@gmail.com/My Drive/portafolio/job_matcher/.venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-alarcon.adrianc@gmail.com/My Drive/portafolio/job_matcher/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-alarcon.adrianc@gmail.com/My Drive/portafolio/job_matcher/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-alarcon.adrianc@gmail.com/My Drive/portafolio/job_matcher/.venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-alarcon.adrianc@gmail.com/My Drive/portafolio/job_matcher/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[21], line 49\u001b[0m, in \u001b[0;36mlemmatize_tokens.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     47\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes_core_news_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nlp(text)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlemma_\n\u001b[0;32m---> 49\u001b[0m df[tokens_col] \u001b[38;5;241m=\u001b[39m df[tokens_col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [\u001b[43mlemmatize_stanza\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[0;32mIn[21], line 34\u001b[0m, in \u001b[0;36mlemmatize_stanza\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlemmatize_stanza\u001b[39m(text):\n\u001b[0;32m---> 34\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m \u001b[43mstanza\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[1;32m     36\u001b[0m     final \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-alarcon.adrianc@gmail.com/My Drive/portafolio/job_matcher/.venv/lib/python3.12/site-packages/stanza/pipeline/core.py:308\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, resources_filepath, proxies, foundation_cache, device, allow_unknown_language, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(curr_processor_config)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;66;03m# try to build processor, throw an exception if there is a requirements issue\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name] \u001b[38;5;241m=\u001b[39m \u001b[43mNAME_TO_PROCESSOR_CLASS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprocessor_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_processor_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessorRequirementsException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# if there was a requirements issue, add it to list which will be printed at end\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     pipeline_reqs_exceptions\u001b[38;5;241m.\u001b[39mappend(e)\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-alarcon.adrianc@gmail.com/My Drive/portafolio/job_matcher/.venv/lib/python3.12/site-packages/stanza/pipeline/lemma_processor.py:32\u001b[0m, in \u001b[0;36mLemmaProcessor.__init__\u001b[0;34m(self, config, pipeline, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pretagged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-alarcon.adrianc@gmail.com/My Drive/portafolio/job_matcher/.venv/lib/python3.12/site-packages/stanza/pipeline/processor.py:193\u001b[0m, in \u001b[0;36mUDProcessor.__init__\u001b[0;34m(self, config, pipeline, device)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_variant\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_up_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# build the final config for the processor\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_up_final_config(config)\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-alarcon.adrianc@gmail.com/My Drive/portafolio/job_matcher/.venv/lib/python3.12/site-packages/stanza/pipeline/lemma_processor.py:58\u001b[0m, in \u001b[0;36mLemmaProcessor._set_up_model\u001b[0;34m(self, config, pipeline, device)\u001b[0m\n\u001b[1;32m     56\u001b[0m lemma_classifier_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(args)\n\u001b[1;32m     57\u001b[0m lemma_classifier_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordvec_pretrain_file\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrain_path\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoundation_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfoundation_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemma_classifier_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlemma_classifier_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-alarcon.adrianc@gmail.com/My Drive/portafolio/job_matcher/.venv/lib/python3.12/site-packages/stanza/models/lemma/trainer.py:38\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, args, vocab, emb_matrix, model_file, device, foundation_cache, lemma_classifier_args)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, emb_matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, foundation_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lemma_classifier_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# load everything from file\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfoundation_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemma_classifier_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;66;03m# build model from scratch\u001b[39;00m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-alarcon.adrianc@gmail.com/My Drive/portafolio/job_matcher/.venv/lib/python3.12/site-packages/stanza/models/lemma/trainer.py:297\u001b[0m, in \u001b[0;36mTrainer.load\u001b[0;34m(self, filename, args, foundation_cache, lemma_classifier_args)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, args, foundation_cache, lemma_classifier_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 297\u001b[0m         checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load model from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(filename))\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-alarcon.adrianc@gmail.com/My Drive/portafolio/job_matcher/.venv/lib/python3.12/site-packages/torch/serialization.py:1487\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1486\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1487\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1494\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-alarcon.adrianc@gmail.com/My Drive/portafolio/job_matcher/.venv/lib/python3.12/site-packages/torch/serialization.py:1754\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1752\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1753\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1754\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1756\u001b[0m deserialized_storage_keys \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mactive_fake_mode() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-alarcon.adrianc@gmail.com/My Drive/portafolio/job_matcher/.venv/lib/python3.12/site-packages/torch/_weights_only_unpickler.py:517\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemo[idx])\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINPUT[\u001b[38;5;241m0\u001b[39m], LONG_BINPUT[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[0;32m--> 517\u001b[0m     i \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINPUT[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative argument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "texto = get_text(\"/Users/aladelca/Downloads/Carlos Adrián Alarcón Delgado alarcon.docx\")\n",
    "df = pd.DataFrame({\"texto\": [texto]})\n",
    "preprocess_text(\n",
    "    df,\n",
    "    \"texto\",\n",
    "    list(set(stopwords.words('spanish')))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
